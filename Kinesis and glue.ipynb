{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63206933-f9d2-4a18-94b6-b82b7de830ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis and Glue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b592d-ce44-4c69-a1cf-9af431f800b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "from pyspark.sql import DataFrame, Row\n",
    "import datetime\n",
    "from awsglue import DynamicFrame\n",
    "\n",
    "# Get job parameters from the command line arguments\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "# Initialize Spark and Glue contexts\n",
    "sc = SparkContext()  # Create a SparkContext\n",
    "glueContext = GlueContext(sc)  # Create a GlueContext to interact with AWS Glue\n",
    "spark = glueContext.spark_session  # Get the Spark session from GlueContext\n",
    "job = Job(glueContext)  # Initialize a Glue job\n",
    "job.init(args['JOB_NAME'], args)  # Start the job with its name\n",
    "\n",
    "# Create a DataFrame from the Kinesis stream\n",
    "dataframe_AmazonKinesis_node1726555925249 = glueContext.create_data_frame.from_options(\n",
    "    connection_type=\"kinesis\",\n",
    "    connection_options={\n",
    "        \"typeOfData\": \"kinesis\",\n",
    "        \"streamARN\": \"arn:aws:kinesis:us-east-1:484907529427:stream/sample_stream\",\n",
    "        \"classification\": \"csv\",\n",
    "        \"startingPosition\": \"earliest\",\n",
    "        \"inferSchema\": \"true\"\n",
    "    },\n",
    "    transformation_ctx=\"dataframe_AmazonKinesis_node1726555925249\"\n",
    ")\n",
    "\n",
    "# Define a function to process each batch of data\n",
    "def processBatch(data_frame, batchId):\n",
    "    if (data_frame.count() > 0):  # Check if the DataFrame has any records\n",
    "        # Convert the DataFrame to a DynamicFrame for Glue compatibility\n",
    "        AmazonKinesis_node1726555925249 = DynamicFrame.fromDF(data_frame, glueContext, \"from_data_frame\")\n",
    "        \n",
    "        # Get the current date and time for partitioning the output\n",
    "        now = datetime.datetime.now()\n",
    "        year = now.year\n",
    "        month = now.month\n",
    "        day = now.day\n",
    "        hour = now.hour\n",
    "\n",
    "        # Construct the S3 output path for the processed data\n",
    "        AmazonS3_node1726556007033_path = (\n",
    "            \"s3://lambda-etl-vp-v4/Error/2025\" +\n",
    "            \"/ingest_year=\" + \"{:0>4}\".format(str(year)) +\n",
    "            \"/ingest_month=\" + \"{:0>2}\".format(str(month)) +\n",
    "            \"/ingest_day=\" + \"{:0>2}\".format(str(day)) +\n",
    "            \"/ingest_hour=\" + \"{:0>2}\".format(str(hour)) + \"/\"\n",
    "        )\n",
    "\n",
    "        # Write the DynamicFrame to S3 in CSV format\n",
    "        AmazonS3_node1726556007033 = glueContext.write_dynamic_frame.from_options(\n",
    "            frame=AmazonKinesis_node1726555925249,\n",
    "            connection_type=\"s3\",\n",
    "            format=\"csv\",\n",
    "            connection_options={\n",
    "                \"path\": AmazonS3_node1726556007033_path,\n",
    "                \"partitionKeys\": []  # No partition keys specified\n",
    "            },\n",
    "            format_options={\n",
    "                \"compression\": \"uncompressed\"  # Write uncompressed files\n",
    "            },\n",
    "            transformation_ctx=\"AmazonS3_node1726556007033\"\n",
    "        )\n",
    "\n",
    "# Process each batch of data from the Kinesis stream\n",
    "glueContext.forEachBatch(\n",
    "    frame=dataframe_AmazonKinesis_node1726555925249,\n",
    "    batch_function=processBatch,\n",
    "    options={\n",
    "        \"windowSize\": \"100 seconds\",  # Define the window size for batching\n",
    "        \"checkpointLocation\": args[\"TempDir\"] + \"/\" + args[\"JOB_NAME\"] + \"/checkpoint/\"  # Specify checkpoint location\n",
    "    }\n",
    ")\n",
    "\n",
    "# Commit the Glue job to finalize the processing\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248dd47-0986-490a-a094-2ff174d1d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defe27c-bbeb-45c7-8d99-2b2b4a535558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KinesisToS3Streaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema of the incoming data (modify according to your actual data)\n",
    "schema = StructType([\n",
    "    StructField(\"column1\", StringType(), True),\n",
    "    StructField(\"column2\", StringType(), True),\n",
    "    # Add more fields as per your data\n",
    "])\n",
    "\n",
    "# Create a streaming DataFrame by reading from the Kinesis stream\n",
    "kinesis_stream = spark.readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", \"sample_stream\") \\\n",
    "    .option(\"region\", \"us-east-1\") \\\n",
    "    .option(\"startingPosition\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Assuming the data is in CSV format, you may need to deserialize it\n",
    "# This step depends on how your data is formatted in Kinesis\n",
    "# For CSV, you can use the following code to split the data:\n",
    "kinesis_data = kinesis_stream \\\n",
    "    .selectExpr(\"CAST(data AS STRING)\") \\\n",
    "    .selectExpr(\"split(data, ',') AS csv_values\") \\\n",
    "    .selectExpr(\"csv_values[0] AS column1\", \"csv_values[1] AS column2\")  # Adjust indices according to your schema\n",
    "\n",
    "# Add a timestamp to each record\n",
    "kinesis_data = kinesis_data.withColumn(\"ingestion_time\", current_timestamp())\n",
    "\n",
    "# Define the output S3 path\n",
    "output_path = \"s3://lambda-etl-vp-v4/Error/2025/ingest_year={}/ingest_month={}/ingest_day={}/ingest_hour={}/\".format(\n",
    "    \"2025\", \"01\", \"01\", \"01\"  # Use dynamic values as necessary\n",
    ")\n",
    "\n",
    "# Write the streaming DataFrame to S3\n",
    "query = kinesis_data.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", output_path) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/kinesis_to_s3\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5396dc-27b5-4c99-8e5b-001fab98b586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
